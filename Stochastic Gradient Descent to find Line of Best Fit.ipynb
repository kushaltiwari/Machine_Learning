{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent to find Line of Best Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the line of Best-Fit\n",
    "\n",
    "The equation of the multi regression line is given by\n",
    "\n",
    "\\begin{equation}\n",
    " \\mathbf{\\hat{y}}_{i}= \\sum_{j=1}^{p} \\mathbf{\\alpha}_{j} \\cdot \\mathbf{X}_{ij} + \\beta  \\quad \\quad i = 1,2 \\dots n\n",
    "\\end{equation}\n",
    "where,\n",
    "> $\\hat{y}_{i}$ is the estimated value of $i^{th}$ observation  <br>\n",
    "> $X_{ij}$ is the $j^{th}$ predictor for the $i^{th}$ observation <br>\n",
    "> $\\alpha_{j}$ is the slope of the line made by $j^{th}$ predictor <br>\n",
    "> $\\beta$ is the y-intercept <br>\n",
    "\n",
    "The equation comprises of $n$ observations of 1 dependent variable $y$ and $p$ independent variables $X$. The matrix form of each component would look like\n",
    "\n",
    "- Observations\n",
    "\\begin{equation}\n",
    "    \\mathbf{y}_{i} = [y_{1}, y_{2}, y_{3}, \\dots, y_{n}]^T\n",
    "\\end{equation}\n",
    "\n",
    "- Predictors\n",
    "\\begin{equation}\n",
    "\\mathbf{X}_{i,j} = \\begin{vmatrix}\n",
    "    x_{1,1} & x_{1,2} & x_{1,3} & \\ldots & x_{1,n} \\\\\n",
    "    x_{2,1} & x_{2,2} & x_{2,3} & \\ldots & x_{2,n} \\\\\n",
    "    \\vdots  & \\vdots  & \\vdots  & \\ddots & \\vdots \\\\\n",
    "    x_{p,1} & x_{p,2} & x_{p,3} & \\ldots & x_{p,n} \\\\\n",
    "    \\end{vmatrix}\n",
    "\\end{equation}\n",
    "- Slopes\n",
    "\\begin{equation}\n",
    "    \\mathbf{\\alpha}_{j} = [\\alpha_{1}, \\alpha_{2}, \\alpha_{3}, \\dots, \\alpha_{p}]\n",
    "\\end{equation}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective of Line of Best Fit\n",
    "\n",
    "Assume $p=1$ for simplicity. i.e. we have only one independent variable. Thereby we reduce the slope expression to $\\alpha$ as we have only one predictor (There is only one slope for one independent variable, hence it doesn't need indexing).\n",
    "We need the values of Slope ($\\mathbf{\\alpha}$) and y-intercept ($\\beta$) to determine equation of the line of best fit. The values of slope and y-intercept should be chosen in such a way that it minimizes the sum of squared deviation between the each observed and estimated value of $y$ (line of bets fit) is minimized. This sum of squared deivation is the error ($e$) we want to minimize as much as possible OR the perpecdicular distance between each observed point and line of best fit is as small as possible.\n",
    "\n",
    "Therefore, minimize the error function and determine the best possible values of Slope ($\\beta$) and y-intercept ($\\beta_{0}$) that is used to build a regression equation using these slope and y-intercept.\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{aligned}\n",
    "         & \\underset{\\alpha, \\quad \\beta}{minimize} \\quad \\sum_{i=0}^{n} ((\\alpha \\cdot \\mathbf{x}_{i}+\\beta) - \\mathbf{y}_{i})^{2}  \\\\\n",
    "         & \\underset{\\alpha, \\quad \\beta}{minimize} \\quad \\sum_{i=0}^{n} (\\mathbf{\\hat{y}}_{i} - \\mathbf{y}_{i})^{2}  \\\\\n",
    "         & \\underset{\\alpha, \\quad \\beta}{minimize} \\quad \\sum_{i=0}^{n} (\\mathbf{e}_{i})^{2}\n",
    "     \\end{aligned}\n",
    "\\end{equation}    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Regression Way\n",
    "\n",
    "### 1.1 y-intercept\n",
    "\n",
    "\\begin{equation}\n",
    "    \\beta = \\alpha \\cdot \\bar{x} + \\bar{y}\n",
    "\\end{equation}    \n",
    "\n",
    "where,\n",
    "> $\\bar{x}$ is the mean of independent variable $\\mathbf{x}$ <br>\n",
    "> $\\bar{y}$ is the mean of independent variable $\\mathbf{y}$\n",
    "\n",
    "This means the regression line passes through the center of gravity ($\\bar{x}$, $\\bar{y}$) of scattered data points. The above equation yields <br><br>\n",
    "<center>If $x_{i} = 0$, then $y_{i}=\\bar{y}$</center>\n",
    "\n",
    "\n",
    "### 1.2 Slope\n",
    "\n",
    "\\begin{equation}\n",
    "    \\alpha = \\frac{\\sum_{i=1}^{n} (\\mathbf{x}_{i} - \\bar{x})(\\mathbf{y}_{i} - \\bar{y}) }{\\sum_{i=1}^{n}(\\mathbf{x}_{i} - \\bar{x})^2 }\n",
    "\\end{equation} \n",
    "\n",
    "In the above equation:\n",
    "\n",
    "    Numerator: Covariance between the variables x and y.\n",
    "    Denominator: Variance of x.\n",
    "    \n",
    "The slope equation tells us that how much variation in the distribution of ($x, y$) is accounted by the variation in distribution of ($x$). The variability in ($x,y$) distribution expalined by variability in ($x$). Therefore the regression line tries to predict the value $y$ given $x$ on the basis of their variability. Slope is the factor that does this prediction.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gradient Descent Way\n",
    "\n",
    "\n",
    "### The Gradient Descent equation\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eq:sample}\n",
    "\\begin{aligned}\n",
    "     x_{i+1} & = x_{i} - \\alpha \\cdot f^{\\prime}(x_{i})  \\\\ \n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "where,\n",
    "> $x_{i+1}$ is the next guess that is closer to the minimum <br>\n",
    "> $x_{i}$ is the initial guess <br>\n",
    "> $\\alpha$ is the step length <br>\n",
    "> $f^{\\prime}(x_{i})$ is the derivative of the function f at point $x_{i}$\n",
    "\n",
    "\n",
    "### The objective of Line of Best Fit (Cost Function)\n",
    "\n",
    "\\begin{equation}\n",
    "   \\underset{\\alpha, \\quad \\beta}{minimize} \\quad \\sum_{i=0}^{n} ((\\alpha \\cdot \\mathbf{x}_{i}+\\beta) - \\mathbf{y}_{i})^{2}\n",
    "\\end{equation}    \n",
    "\n",
    "\n",
    "### Formulate Line of Best Fit Problem in form of Gradient Descent\n",
    "\n",
    "We'll use $s$ to denote the step function of gradient descent.\n",
    "\n",
    "\\begin{equation}\n",
    "   \\begin{vmatrix}\n",
    "       \\alpha_{i+1} \\\\\n",
    "       \\beta_{i+1}\n",
    "   \\end{vmatrix} = \\begin{vmatrix}\n",
    "                       \\alpha_{i} \\\\\n",
    "                       \\beta_{i}\n",
    "                   \\end{vmatrix}  - s \\cdot \\begin{vmatrix}\n",
    "                                               \\frac{d}{d\\alpha} \\alpha_{i} \\\\\n",
    "                                               \\frac{d}{d\\beta} \\beta_{i}\n",
    "                                             \\end{vmatrix}   \n",
    "\\end{equation}   \n",
    "\n",
    "      DERIVATIVES\n",
    "      \n",
    "\\begin{equation}\n",
    "  f(\\alpha, \\beta) = \\sum_{i=0}^{n} ((\\alpha \\cdot \\mathbf{x}_{i}+\\beta) - \\mathbf{y}_{i})^{2}\n",
    "\\end{equation} \n",
    "\n",
    "Note: The cost function is a quadratic function.\n",
    "      \n",
    "\\begin{equation}\n",
    "  \\frac{d}{d\\alpha} = 2((\\alpha \\cdot \\mathbf{x}_{i}+\\beta) - \\mathbf{y}_{i}) \\cdot x_{i}  \\quad i \\in rand(n)  \n",
    "\\end{equation}    \n",
    "\n",
    "\\begin{equation}\n",
    "  \\frac{d}{d\\beta} = 2((\\alpha \\cdot \\mathbf{x}_{i}+\\beta) - \\mathbf{y}_{i})  \\quad i \\in rand(n)  \n",
    "\\end{equation}   \n",
    "\n",
    "In case of stochastic gradient descent, any one $i^{th}$ out of $n$ data is chosen to calculate the derivative instead to summing over all data points. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALGORITHM\n",
    "\n",
    "1. Identify independent varuiable ($x$) and dependent variable ($y$) in the dataset.\n",
    "2. Choose the initial slope $\\alpha_{0}$, y-intercept $\\beta_{0}$, step function $s$, $\\epsilon$ and MAX_ITER.\n",
    "3. Calculate:\n",
    ">3.1 The current objective magnitude $f(\\alpha_{i}, \\beta_{i})$. <br>\n",
    ">3.2 The derivatives $f^{\\prime} (\\alpha)$  and $f^{\\prime} (\\beta)$ for any one random data point. <br>\n",
    ">3.3 Solve for $\\alpha_{i+1}$ and $\\beta_{i+1}$. <br> \n",
    ">3.4 The new objective magnitude $f(\\alpha_{i+1}, \\beta_{i+1})$ using $\\alpha_{i+1}$ and $\\beta_{i+1}$.\n",
    "\n",
    "4. Check if the objective is increasing ($f(\\alpha_{i+1}, \\beta_{i+1})$ > $f(\\alpha_{i}, \\beta_{i})$).\n",
    ">4.1 If yes, reduce step function ($s$). Repeat from 3. <br>\n",
    ">4.2 If No, Update $\\alpha_{i}$ as $\\alpha_{i} = \\alpha_{i+1}$ and update $\\beta_{i}$ as $\\beta_{i} = \\beta_{i+1}$. Repeat from 3. \n",
    "\n",
    "5. Check if $|f(\\alpha_{i}, \\beta_{i}) - f(\\alpha_{i+1}, \\beta_{i+1})| < \\epsilon$ where epsilon is the difference between the two values that is acceptable for convergence.\n",
    ">5.1 If converged, $\\alpha_{i}$ and $\\beta_{i}$ are values that best minimizes the cost function $f(\\alpha, \\beta)$ and gives the best possible values of slope and y-intercept for the line of best fit. <br>\n",
    "\n",
    "The algorithm converges when $\\alpha$ and $\\beta$ gives the minimum value of the cost function $f(\\alpha, \\beta)$ for small enough $s$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from matplotlib import pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "data = pd.read_csv('D:/Briefcase/Files/Development/Python/Dataset/regression/train.csv') \n",
    "# Create X and y variables\n",
    "X = data.iloc[0:49,0]\n",
    "y = data.iloc[0:49,1]\n",
    "\n",
    "# Data 1\n",
    "# X = np.array([24, 50, 15, 38, 87, 36])\n",
    "# y = np.array([21.54, 47.46, 17.21, 36.58, 87.28, 32.46])\n",
    "\n",
    "# Data 2\n",
    "# X = np.array([1, 2, 2, 3])\n",
    "# y = np.array([1, 1, 2, 2])\n",
    "\n",
    "# Initialization\n",
    "step_function_initial = 0.2\n",
    "epsilon = 1e-5\n",
    "alpha_beta_initial = np.array([1, 1]) # (\\alpha, \\beta)\n",
    "MAX_ITER = 500\n",
    "N = X.size\n",
    "\n",
    "# Plot Variables\n",
    "global store_alpha_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine Objective Function\n",
    "def get_objective(alpha, beta):\n",
    "    return sum((y - (alpha * X + beta))**2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict y\n",
    "def predict_y(alpha, beta):\n",
    "    return alpha*X + beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Derivates\n",
    "def get_derivative(alpha, beta):\n",
    "    i = np.random.choice(N,1)\n",
    "    dalpha = int(2*(( (X[i]*alpha) + beta - y[i] ) * X[i]))\n",
    "    dbeta = int(2*( (X[i] *alpha) + beta - y[i] ))\n",
    "    return np.array([dalpha, dbeta])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alculate Initial Objective\n",
    "objective_initial = get_objective(alpha_beta_initial[0], alpha_beta_initial[1])\n",
    "\n",
    "# Solve for alpha and beta\n",
    "def minimize_cost_function():\n",
    "    global store_alpha_beta\n",
    "    store_alpha_beta = []\n",
    "    main_iter = 0\n",
    "    alpha_beta_current = alpha_beta_initial\n",
    "    step_function = step_function_initial\n",
    "          \n",
    "    while(main_iter < MAX_ITER):\n",
    "        main_iter += 1\n",
    "            \n",
    "        # Calculate Objective Function\n",
    "        objective_current = get_objective(alpha_beta_current[0], alpha_beta_current[1])            \n",
    "            \n",
    "        # Calculate Derivatives\n",
    "        dalpha_dbeta = get_derivative(alpha_beta_current[0], alpha_beta_current[1])\n",
    "        alpha_beta_new = alpha_beta_current - step_function*dalpha_dbeta\n",
    "        print(alpha_beta_current[0], alpha_beta_current[1], 'Derivative', dalpha_dbeta[0], dalpha_dbeta[1])\n",
    "            \n",
    "        # Calculate updated Objective Function\n",
    "        objective_new = get_objective(alpha_beta_new[0], alpha_beta_new[1])\n",
    "            \n",
    "        # Check if Objective Function is decreasing, Else decrease Step Function \n",
    "        if objective_new > objective_current:\n",
    "            step_function = step_function*0.95\n",
    "            continue\n",
    "        else:\n",
    "            alpha_beta_current = alpha_beta_new\n",
    "            store_alpha_beta.append(alpha_beta_current)\n",
    "                     \n",
    "        # Check for Convergence\n",
    "        convergence = abs(objective_current - objective_new) < epsilon   \n",
    "        if(convergence):\n",
    "            print('CONVERGED')\n",
    "            return alpha_beta_current[0], alpha_beta_current[1], step_function\n",
    "\n",
    "# Calculate Regression using scipy\n",
    "sci_slope, sci_intercept, r_value, p_value, std_err = stats.linregress(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 Derivative 225 3\n",
      "1 1 Derivative 42 1\n",
      "1 1 Derivative 227 3\n",
      "1 1 Derivative 90 4\n",
      "1 1 Derivative 326 9\n",
      "1 1 Derivative 165 6\n",
      "1 1 Derivative 200 4\n",
      "1 1 Derivative 353 7\n",
      "1 1 Derivative -24 -1\n",
      "1 1 Derivative 225 3\n",
      "1 1 Derivative -81 -4\n",
      "1 1 Derivative 524 7\n",
      "1 1 Derivative -341 -5\n",
      "1 1 Derivative 708 13\n",
      "1 1 Derivative -110 -1\n",
      "1 1 Derivative 240 6\n",
      "1 1 Derivative 46 1\n",
      "1 1 Derivative 353 7\n",
      "1 1 Derivative 525 5\n",
      "1 1 Derivative 110 6\n",
      "1 1 Derivative 0 1\n",
      "1.0 0.9283028155182916 Derivative 65 2\n",
      "1.0 0.9283028155182916 Derivative 514 7\n",
      "1.0 0.9283028155182916 Derivative -676 -7\n",
      "1.0 0.9283028155182916 Derivative 319 3\n",
      "1.0 0.9283028155182916 Derivative -16 0\n",
      "1.0 0.9283028155182916 Derivative 196 7\n",
      "1.0 0.9283028155182916 Derivative 468 8\n",
      "1.0 0.9283028155182916 Derivative -92 -1\n",
      "1.0 0.9283028155182916 Derivative 182 11\n",
      "1.0 0.9283028155182916 Derivative 235 6\n",
      "1.0 0.9283028155182916 Derivative 235 6\n",
      "1.0 0.9283028155182916 Derivative 0 4\n",
      "1.0 0.7651773549216594 Derivative 28 0\n",
      "1.0 0.7651773549216594 Derivative 0 4\n",
      "1.0 0.6102081673548587 Derivative 455 5\n",
      "1.0 0.6102081673548587 Derivative 467 6\n",
      "1.0 0.6102081673548587 Derivative -19 -4\n",
      "1.0 0.6102081673548587 Derivative 98 6\n",
      "1.0 0.6102081673548587 Derivative 433 8\n",
      "1.0 0.6102081673548587 Derivative -13 -2\n",
      "1.0 0.6102081673548587 Derivative 23 0\n",
      "1.0 0.6102081673548587 Derivative 22 3\n",
      "1.0 0.6102081673548587 Derivative 467 6\n",
      "1.0 0.6102081673548587 Derivative 49 1\n",
      "1.0 0.6102081673548587 Derivative 98 6\n",
      "1.0 0.6102081673548587 Derivative -89 -4\n",
      "1.0 0.6102081673548587 Derivative 49 1\n",
      "1.0 0.6102081673548587 Derivative 467 6\n",
      "1.0 0.6102081673548587 Derivative 180 3\n",
      "1.0 0.6102081673548587 Derivative 12 0\n",
      "1.0 0.6102081673548587 Derivative -123 -2\n",
      "1.0 0.6102081673548587 Derivative 1010 14\n",
      "1.0 0.6102081673548587 Derivative 43 3\n",
      "1.0 0.6102081673548587 Derivative 146 6\n",
      "1.0 0.6102081673548587 Derivative 153 4\n",
      "1.0 0.6102081673548587 Derivative 452 4\n",
      "1.0 0.6102081673548587 Derivative 0 0\n",
      "CONVERGED\n"
     ]
    }
   ],
   "source": [
    "# Run the Algorithm\n",
    "try:\n",
    "     slope, intercept, step_function = minimize_cost_function() \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Objective: 417.1408337908109\n",
      "Final Objective: 369.8981696592472\n",
      "\n",
      "Calulated Slope: 1.0 / SCIPY Slope: 0.9933000023086673\n",
      "Calulated Intercept: 0.6102081673548587 / SCIPY Intercept: -0.1498177287176219\n",
      "\n",
      "Best Line Fit: y = 1.0 X + 0.6102081673548587\n",
      "\n",
      "Step Function 0.012534432653795658\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c4adbfe710>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c4ac46b208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate Objective Function\n",
    "objective_final = get_objective(slope, intercept)\n",
    "print('Initial Objective:', objective_initial)\n",
    "print('Final Objective:', objective_final)\n",
    "\n",
    "# SCIPY Comparison\n",
    "print('\\nCalulated Slope:', slope, '/' ,'SCIPY Slope:', sci_slope)\n",
    "print('Calulated Intercept:',intercept, '/', 'SCIPY Intercept:', sci_intercept)\n",
    "\n",
    "# Regression Line Equation\n",
    "print('\\nBest Line Fit: y = {0} X + {1}'.format(slope, intercept))\n",
    "\n",
    "# Step Function\n",
    "print('\\nStep Function', step_function)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "# plt.rcParams.update({'lines.linewidth':2, 'lines.linestyle':'--'})\n",
    "ax.set(xlabel='X', ylabel='Y', title='Line of Best Fit')\n",
    "ax.scatter(X, y)\n",
    "for alpha_beta in store_alpha_beta:\n",
    "    ax.plot(X, predict_y(alpha_beta[0], alpha_beta[1]), color = 'c', linewidth = 1)\n",
    "ax.plot(X, predict_y(slope, intercept), color = 'r', linewidth = 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
